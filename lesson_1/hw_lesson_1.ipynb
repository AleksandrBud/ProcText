{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
    "Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'\n",
    "\n",
    "2) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function).\n",
    "\n",
    "3) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "4) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "5) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "7) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "8) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "9) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "10) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "11) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "12) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "13) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "14) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "15) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‑)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "from html.parser import HTMLParser\n",
    "# import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      "id       49159 non-null int64\n",
      "label    31962 non-null float64\n",
      "tweet    49159 non-null object\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combine_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6         @user camping tomorrow @user @user @user @use...\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9         @user @user welcome here !  i'm   it's so #gr...\n",
       "10        â #ireland consumer price index (mom) climb...\n",
       "11       we are so selfish. #orlando #standwithorlando ...\n",
       "12       i get to see my daddy today!!   #80days #getti...\n",
       "13       @user #cnn calls #michigan middle school 'buil...\n",
       "14       no comment!  in #australia   #opkillingbay #se...\n",
       "15       ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16       i am thankful for having a paner. #thankful #p...\n",
       "17                                  retweet if you agree! \n",
       "18       its #friday! ð smiles all around via ig use...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       #euro2016 people blaming ha for conceded goal ...\n",
       "21       sad little dude..   #badday #coneofshame #cats...\n",
       "22       product of the day: happy man #wine tool  who'...\n",
       "23         @user @user lumpy says i am a . prove it lumpy.\n",
       "24        @user #tgif   #ff to my #gamedev #indiedev #i...\n",
       "25       beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26        @user all #smiles when #media is   !! ðð...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28             happy father's day @user ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "49129    people do anything for fucking attention nowad...\n",
       "49130    creative bubble got burst ð¢ looking forward...\n",
       "49131    tomorrow is gonna be a big day! we are going t...\n",
       "49132    i am thankful for baby giggles. #thankful #pos...\n",
       "49133    #model   i love u take with u all the time in ...\n",
       "49134    in life u will grow to learn some pple will wo...\n",
       "49135    ði was the storm,you were the rain. togethe...\n",
       "49136    lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "49137    spread love not hateâ¤ï¸ðððð #pr...\n",
       "49138       @user @user are the most racist pay ever!!!!! \n",
       "49139    i am thankful for children. #thankful #positiv...\n",
       "49140    liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "49141    #bakersfield   rooster simulation: i want to c...\n",
       "49142    por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "49143    @user hell yeah what a great surprise for your...\n",
       "49144    when ur the joke ur defensive towards everythi...\n",
       "49145    #enjoying the #evening #sun in my #bedroom â¨...\n",
       "49146    tonight on @user from 9pm gmt  you can here a ...\n",
       "49147    today is a good day for excercise #imready #so...\n",
       "49148    good night with a tea and music âï¸ðð...\n",
       "49149    loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "49150    black professor demonizes, proposes nazi style...\n",
       "49151    learn how to think positive.  #positive   #ins...\n",
       "49152    we love the pretty, happy and fresh you! #teen...\n",
       "49153    2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "49154    thought factory: left-right polarisation! #tru...\n",
       "49155    feeling like a mermaid ð #hairflip #neverre...\n",
       "49156    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157    happy, at work conference: right mindset leads...\n",
       "49158    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTMLParser.unescape('&lt', combine_df['tweet'])\n",
    "HTMLParser.unescape('&gt', combine_df['tweet'])\n",
    "HTMLParser.unescape('&amp', combine_df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    return re.sub('@user', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          when a father is dysfunctional and is so sel...\n",
       "1          thanks for #lyft credit i can't use cause th...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6                         camping tomorrow        dannyâ¦\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9                    welcome here !  i'm   it's so #gr8 ! \n",
       "10        â #ireland consumer price index (mom) climb...\n",
       "11       we are so selfish. #orlando #standwithorlando ...\n",
       "12       i get to see my daddy today!!   #80days #getti...\n",
       "13        #cnn calls #michigan middle school 'build the...\n",
       "14       no comment!  in #australia   #opkillingbay #se...\n",
       "15       ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16       i am thankful for having a paner. #thankful #p...\n",
       "17                                  retweet if you agree! \n",
       "18       its #friday! ð smiles all around via ig use...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       #euro2016 people blaming ha for conceded goal ...\n",
       "21       sad little dude..   #badday #coneofshame #cats...\n",
       "22       product of the day: happy man #wine tool  who'...\n",
       "23                     lumpy says i am a . prove it lumpy.\n",
       "24         #tgif   #ff to my #gamedev #indiedev #indieg...\n",
       "25       beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26         all #smiles when #media is   !! ðð #pr...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28                  happy father's day  ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "49129    people do anything for fucking attention nowad...\n",
       "49130    creative bubble got burst ð¢ looking forward...\n",
       "49131    tomorrow is gonna be a big day! we are going t...\n",
       "49132    i am thankful for baby giggles. #thankful #pos...\n",
       "49133    #model   i love u take with u all the time in ...\n",
       "49134    in life u will grow to learn some pple will wo...\n",
       "49135    ði was the storm,you were the rain. togethe...\n",
       "49136    lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "49137    spread love not hateâ¤ï¸ðððð #pr...\n",
       "49138                   are the most racist pay ever!!!!! \n",
       "49139    i am thankful for children. #thankful #positiv...\n",
       "49140    liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "49141    #bakersfield   rooster simulation: i want to c...\n",
       "49142    por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "49143     hell yeah what a great surprise for your pres...\n",
       "49144    when ur the joke ur defensive towards everythi...\n",
       "49145    #enjoying the #evening #sun in my #bedroom â¨...\n",
       "49146    tonight on  from 9pm gmt  you can here a speci...\n",
       "49147    today is a good day for excercise #imready #so...\n",
       "49148    good night with a tea and music âï¸ðð...\n",
       "49149    loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "49150    black professor demonizes, proposes nazi style...\n",
       "49151    learn how to think positive.  #positive   #ins...\n",
       "49152    we love the pretty, happy and fresh you! #teen...\n",
       "49153    2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "49154    thought factory: left-right polarisation! #tru...\n",
       "49155    feeling like a mermaid ð #hairflip #neverre...\n",
       "49156    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157    happy, at work conference: right mindset leads...\n",
       "49158    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Изменим регистр твитов на нижний с помощью .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 ms, sys: 4.43 ms, total: 254 ms\n",
      "Wall time: 265 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          when a father is dysfunctional and is so sel...\n",
       "1          thanks for #lyft credit i can't use cause th...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6                         camping tomorrow        dannyâ¦\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9                    welcome here !  i'm   it's so #gr8 ! \n",
       "10        â #ireland consumer price index (mom) climb...\n",
       "11       we are so selfish. #orlando #standwithorlando ...\n",
       "12       i get to see my daddy today!!   #80days #getti...\n",
       "13        #cnn calls #michigan middle school 'build the...\n",
       "14       no comment!  in #australia   #opkillingbay #se...\n",
       "15       ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16       i am thankful for having a paner. #thankful #p...\n",
       "17                                  retweet if you agree! \n",
       "18       its #friday! ð smiles all around via ig use...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       #euro2016 people blaming ha for conceded goal ...\n",
       "21       sad little dude..   #badday #coneofshame #cats...\n",
       "22       product of the day: happy man #wine tool  who'...\n",
       "23                     lumpy says i am a . prove it lumpy.\n",
       "24         #tgif   #ff to my #gamedev #indiedev #indieg...\n",
       "25       beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26         all #smiles when #media is   !! ðð #pr...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28                  happy father's day  ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "49129    people do anything for fucking attention nowad...\n",
       "49130    creative bubble got burst ð¢ looking forward...\n",
       "49131    tomorrow is gonna be a big day! we are going t...\n",
       "49132    i am thankful for baby giggles. #thankful #pos...\n",
       "49133    #model   i love u take with u all the time in ...\n",
       "49134    in life u will grow to learn some pple will wo...\n",
       "49135    ði was the storm,you were the rain. togethe...\n",
       "49136    lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "49137    spread love not hateâ¤ï¸ðððð #pr...\n",
       "49138                   are the most racist pay ever!!!!! \n",
       "49139    i am thankful for children. #thankful #positiv...\n",
       "49140    liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "49141    #bakersfield   rooster simulation: i want to c...\n",
       "49142    por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "49143     hell yeah what a great surprise for your pres...\n",
       "49144    when ur the joke ur defensive towards everythi...\n",
       "49145    #enjoying the #evening #sun in my #bedroom â¨...\n",
       "49146    tonight on  from 9pm gmt  you can here a speci...\n",
       "49147    today is a good day for excercise #imready #so...\n",
       "49148    good night with a tea and music âï¸ðð...\n",
       "49149    loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "49150    black professor demonizes, proposes nazi style...\n",
       "49151    learn how to think positive.  #positive   #ins...\n",
       "49152    we love the pretty, happy and fresh you! #teen...\n",
       "49153    2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "49154    thought factory: left-right polarisation! #tru...\n",
       "49155    feeling like a mermaid ð #hairflip #neverre...\n",
       "49156    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157    happy, at work conference: right mindset leads...\n",
       "49158    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_word(text, word_dict, mode=1):\n",
    "    if mode == 1:\n",
    "        for key in word_dict:\n",
    "            text = re.sub(key, word_dict[key], text)\n",
    "        return text\n",
    "    elif mode == 2:\n",
    "        ret = ''\n",
    "        for word in text.split():\n",
    "            ret = ret + change_word_2(word, word_dict) + ' '\n",
    "        return ret.strip()\n",
    "    else:\n",
    "        return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_word_2(word, word_dict):\n",
    "    if word_dict.get(word):\n",
    "        return(word_dict.get(word))\n",
    "    else:\n",
    "        return(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 6.5 ms, total: 15.3 s\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          when a father is dysfunctional and is so sel...\n",
       "1          thanks for #lyft credit i cannot use cause t...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "5        [2/2] huge fan fare and big talking before the...\n",
       "6                         camping tomorrow        dannyâ¦\n",
       "7        the next school year is the year for exams.ð...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9           welcome here !  I am   it has / it is so #g...\n",
       "10        â #ireland consumer price index (mom) climb...\n",
       "11       we are so selfish. #orlando #standwithorlando ...\n",
       "12       i get to see my daddy today!!   #80days #getti...\n",
       "13        #cnn calls #michigan middle school 'build the...\n",
       "14       no comment!  in #australia   #opkillingbay #se...\n",
       "15       ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16       i am thankful for having a paner. #thankful #p...\n",
       "17                                  retweet if you agree! \n",
       "18       its #friday! ð smiles all around via ig use...\n",
       "19       as we all know, essential oils are not made of...\n",
       "20       #euro2016 people blaming ha for conceded goal ...\n",
       "21       sad little dude..   #badday #coneofshame #cats...\n",
       "22       product of the day: happy man #wine tool  who ...\n",
       "23                     lumpy says i am a . prove it lumpy.\n",
       "24         #tgif   #ff to my #gamedev #indiedev #indieg...\n",
       "25       beautiful sign by vendor 80 for $45.00!! #upsi...\n",
       "26         all #smiles when #media is   !! ðð #pr...\n",
       "27       we had a great panel on the mediatization of t...\n",
       "28                  happy father's day  ðððð  \n",
       "29       50 people went to nightclub to have a good nig...\n",
       "                               ...                        \n",
       "49129    people do anything for fucking attention nowad...\n",
       "49130    creative bubble got burst ð¢ looking forward...\n",
       "49131    tomorrow is gonna be a big day! we are going t...\n",
       "49132    i am thankful for baby giggles. #thankful #pos...\n",
       "49133    #model   i love u take with u all the time in ...\n",
       "49134    in life u will grow to learn some pple will wo...\n",
       "49135    ði was the storm,you were the rain. togethe...\n",
       "49136    lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "49137    spread love not hateâ¤ï¸ðððð #pr...\n",
       "49138                   are the most racist pay ever!!!!! \n",
       "49139    i am thankful for children. #thankful #positiv...\n",
       "49140    liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "49141    #bakersfield   rooster simulation: i want to c...\n",
       "49142    por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "49143     hell yeah what a great surprise for your pres...\n",
       "49144    when ur the joke ur defensive towards everythi...\n",
       "49145    #enjoying the #evening #sun in my #bedroom â¨...\n",
       "49146    tonight on  from 9pm gmt  you can here a speci...\n",
       "49147    today is a good day for excercise #imready #so...\n",
       "49148    good night with a tea and music âï¸ðð...\n",
       "49149    loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "49150    black professor demonizes, proposes nazi style...\n",
       "49151    learn how to think positive.  #positive   #ins...\n",
       "49152    we love the pretty, happy and fresh you! #teen...\n",
       "49153    2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "49154    thought factory: left-right polarisation! #tru...\n",
       "49155    feeling like a mermaid ð #hairflip #neverre...\n",
       "49156    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157    happy, at work conference: right mindset leads...\n",
       "49158    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 36.8 ms, total: 27.5 s\n",
      "Wall time: 27.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          when a fatheare is dysfyounctional and is so...\n",
       "1          thanks foare #lyft careedit i cannot youse c...\n",
       "2                                  bihday yoyouare majesty\n",
       "3        #model   i love you take with you all the time...\n",
       "4            factsgareinyouide: society now    #motivation\n",
       "5        [2/2] hyougareine fan faaree and bigarein talk...\n",
       "6                campingarein tomoareareow        dannyâ¦\n",
       "7        the next school yeaare is the yeaare foare exa...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9           welcome hearee !  I am   it has / it is so ...\n",
       "10        â #iareeland consyoumeare pareI seee index ...\n",
       "11       we aaree so selfish. #oarelando #standwithoare...\n",
       "12       i gareinet to see my daddy today!!   #80days #...\n",
       "13        #cnn calls #mI seehigareinan middle school 'b...\n",
       "14       no comment!  in #ayoustarealia   #opkillingare...\n",
       "15       oyouch...jyounioare is angareinareyð#garein...\n",
       "16       i am thankfyoul foare havingarein a paneare. #...\n",
       "17                       areetweet if yoyou agareinareee! \n",
       "18       its #fareiday! ð smiles all aareoyound via ...\n",
       "19       as we all know, essential oils aaree not made ...\n",
       "20       #eyouareo2016 people blamingarein ha foare con...\n",
       "21       sad little dyoude..   #badday #coneofshame #ca...\n",
       "22       pareodyouct of the day: happy man #wine tool  ...\n",
       "23               lyoumpy says i am a . pareove it lyoumpy.\n",
       "24         #tgareinif   #ff to my #gareinamedev #indied...\n",
       "25       beayoutifyoul sigareinn by vendoare 80 foare $...\n",
       "26         all #smiles when #media is   !! ðð #pa...\n",
       "27       we had a gareinareeat panel on the mediatizati...\n",
       "28                happy fatheare's day  ðððð  \n",
       "29       50 people went to nigareinhtclyoub to have a g...\n",
       "                               ...                        \n",
       "49129    people do anythingarein foare fyouckingarein a...\n",
       "49130    careeative byoubble gareinot byouarest ð¢ lo...\n",
       "49131    tomoareareow is gareinonna be a bigarein day! ...\n",
       "49132    i am thankfyoul foare baby gareinigareingarein...\n",
       "49133    #model   i love you take with you all the time...\n",
       "49134    in life you will gareinareow to leaaren some p...\n",
       "49135    ði was the stoarem,yoyou wearee the areain....\n",
       "49136    lovelgareinq -  bareoken ep via   #arenb #love...\n",
       "49137    spareead love not hateâ¤ï¸ðððð #...\n",
       "49138             aaree the most areacist pay eveare!!!!! \n",
       "49139    i am thankfyoul foare chilateare, dyoudeareen....\n",
       "49140    livearepool â¤ï¸ð¬ð§ #walk #livearepool ...\n",
       "49141    #back at the keyboaaredearesfielateare, dyoude...\n",
       "49142    poare do sol ó¾â¤ï¸#instagareinood #beayou...\n",
       "49143     hell yeah what a gareinareeat syouarepareise ...\n",
       "49144    when youare the joke youare defensive towaared...\n",
       "49145    #enjoyingarein the #eveningarein #syoun in my ...\n",
       "49146    tonigareinht on  fareom 9pm gareinmt  yoyou ca...\n",
       "49147    today is a gareinood day foare excearecise #im...\n",
       "49148    gareinood nigareinht with a tea and myousI see...\n",
       "49149    lovingarein lifeðºð¸âï¸ð  #careeatey...\n",
       "49150    black pareofessoare demonizes, pareoposes nazi...\n",
       "49151    leaaren how to think positive.  #positive   #i...\n",
       "49152    we love the pareetty, happy and fareesh yoyou!...\n",
       "49153    2_damn_tyouff-areyouff_myouff__techno_city-(ng...\n",
       "49154    thoyougareinht factoarey: left-areigareinht po...\n",
       "49155    feelingarein like a mearemaid ð #haiareflip...\n",
       "49156    #hillaarey #campaigareinned today in #ohio((oh...\n",
       "49157    happy, at woarek confeareence: areigareinht mi...\n",
       "49158    my   songarein \"so gareinlad\" fareee download!...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    ret = change_word(ret, emoticon_dict, 2)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 s, sys: 88.8 ms, total: 28.6 s\n",
      "Wall time: 29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        when a fatheare is dysfyounctional and is so s...\n",
       "1        thanks foare #lyft careedit i cannot youse cay...\n",
       "2                                  bihday yoyouare majesty\n",
       "3        #model i love you take with you all the time i...\n",
       "4               factsgareinyouide: society now #motivation\n",
       "5        [2/2] hyougareine fan faaree and bigarein talk...\n",
       "6                       campingarein tomoareareow dannyâ¦\n",
       "7        the next school yeaare is the yeaare foare exa...\n",
       "8        we won!!! love the land!!! #allin #cavs #champ...\n",
       "9        welcome hearee ! I am it has / it is so #garei...\n",
       "10       â #iareeland consyoumeare pareI seee index (...\n",
       "11       we aaree so selfish. #oarelando #standwithoare...\n",
       "12       i gareinet to see my daddy today!! #80days #ga...\n",
       "13       #cnn calls #mI seehigareinan middle school 'by...\n",
       "14       no comment! in #ayoustarealia #opkillingareinb...\n",
       "15       oyouch...jyounioare is angareinareyð#garein...\n",
       "16       i am thankfyoul foare havingarein a paneare. #...\n",
       "17                        areetweet if yoyou agareinareee!\n",
       "18       its #fareiday! ð smiles all aareoyound via ...\n",
       "19       as we all know, essential oils aaree not made ...\n",
       "20       #eyouareo2016 people blamingarein ha foare con...\n",
       "21       sad little dyoude.. #badday #coneofshame #cats...\n",
       "22       pareodyouct of the day: happy man #wine tool w...\n",
       "23               lyoumpy says i am a . pareove it lyoumpy.\n",
       "24       #tgareinif #ff to my #gareinamedev #indiedev #...\n",
       "25       beayoutifyoul sigareinn by vendoare 80 foare $...\n",
       "26       all #smiles when #media is !! ðð #parees...\n",
       "27       we had a gareinareeat panel on the mediatizati...\n",
       "28                   happy fatheare's day ðððð\n",
       "29       50 people went to nigareinhtclyoub to have a g...\n",
       "                               ...                        \n",
       "49129    people do anythingarein foare fyouckingarein a...\n",
       "49130    careeative byoubble gareinot byouarest ð¢ lo...\n",
       "49131    tomoareareow is gareinonna be a bigarein day! ...\n",
       "49132    i am thankfyoul foare baby gareinigareingarein...\n",
       "49133    #model i love you take with you all the time i...\n",
       "49134    in life you will gareinareow to leaaren some p...\n",
       "49135    ði was the stoarem,yoyou wearee the areain....\n",
       "49136    lovelgareinq - bareoken ep via #arenb #love #h...\n",
       "49137    spareead love not hateâ¤ï¸ðððð #...\n",
       "49138              aaree the most areacist pay eveare!!!!!\n",
       "49139    i am thankfyoul foare chilateare, dyoudeareen....\n",
       "49140    livearepool â¤ï¸ð¬ð§ #walk #livearepool ...\n",
       "49141    #back at the keyboaaredearesfielateare, dyoude...\n",
       "49142    poare do sol ó¾â¤ï¸#instagareinood #beayou...\n",
       "49143    hell yeah what a gareinareeat syouarepareise f...\n",
       "49144    when youare the joke youare defensive towaared...\n",
       "49145    #enjoyingarein the #eveningarein #syoun in my ...\n",
       "49146    tonigareinht on fareom 9pm gareinmt yoyou can ...\n",
       "49147    today is a gareinood day foare excearecise #im...\n",
       "49148    gareinood nigareinht with a tea and myousI see...\n",
       "49149    lovingarein lifeðºð¸âï¸ð #careeateyo...\n",
       "49150    black pareofessoare demonizes, pareoposes nazi...\n",
       "49151    leaaren how to think positive. #positive #inst...\n",
       "49152    we love the pareetty, happy and fareesh yoyou!...\n",
       "49153    2_damn_tyouff-areyouff_myouff__techno_city-(ng...\n",
       "49154    thoyougareinht factoarey: left-areigareinht po...\n",
       "49155    feelingarein like a mearemaid ð #haiareflip...\n",
       "49156    #hillaarey #campaigareinned today in #ohio((oh...\n",
       "49157    happy, at woarek confeareence: areigareinht mi...\n",
       "49158    my songarein \"so gareinlad\" fareee download! #...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    ret = change_word(ret, emoticon_dict, 2)\n",
    "    ret = re.sub(r'[^\\w\\s]', '', ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 61.6 ms, total: 29 s\n",
      "Wall time: 29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        when a fatheare is dysfyounctional and is so s...\n",
       "1        thanks foare lyft careedit i cannot youse cayo...\n",
       "2                                  bihday yoyouare majesty\n",
       "3        model i love you take with you all the time in...\n",
       "4                 factsgareinyouide society now motivation\n",
       "5        22 hyougareine fan faaree and bigarein talking...\n",
       "6                         campingarein tomoareareow dannyâ\n",
       "7        the next school yeaare is the yeaare foare exa...\n",
       "8        we won love the land allin cavs champions clev...\n",
       "9        welcome hearee  I am it has  it is so gareinare8 \n",
       "10       â iareeland consyoumeare pareI seee index mom ...\n",
       "11       we aaree so selfish oarelando standwithoarelan...\n",
       "12       i gareinet to see my daddy today 80days garein...\n",
       "13       cnn calls mI seehigareinan middle school byoui...\n",
       "14       no comment in ayoustarealia opkillingareinbay ...\n",
       "15       oyouchjyounioare is angareinareyðgareinot7 jyo...\n",
       "16       i am thankfyoul foare havingarein a paneare th...\n",
       "17                         areetweet if yoyou agareinareee\n",
       "18       its fareiday ð smiles all aareoyound via igare...\n",
       "19       as we all know essential oils aaree not made o...\n",
       "20       eyouareo2016 people blamingarein ha foare conc...\n",
       "21       sad little dyoude badday coneofshame cats piss...\n",
       "22       pareodyouct of the day happy man wine tool who...\n",
       "23                 lyoumpy says i am a  pareove it lyoumpy\n",
       "24       tgareinif ff to my gareinamedev indiedev indie...\n",
       "25       beayoutifyoul sigareinn by vendoare 80 foare 4...\n",
       "26       all smiles when media is  ðð pareessconfeareen...\n",
       "27       we had a gareinareeat panel on the mediatizati...\n",
       "28                                happy fatheares day ðððð\n",
       "29       50 people went to nigareinhtclyoub to have a g...\n",
       "                               ...                        \n",
       "49129    people do anythingarein foare fyouckingarein a...\n",
       "49130    careeative byoubble gareinot byouarest ð looki...\n",
       "49131    tomoareareow is gareinonna be a bigarein day w...\n",
       "49132    i am thankfyoul foare baby gareinigareingarein...\n",
       "49133    model i love you take with you all the time in...\n",
       "49134    in life you will gareinareow to leaaren some p...\n",
       "49135    ði was the stoaremyoyou wearee the areain toga...\n",
       "49136    lovelgareinq  bareoken ep via arenb love heaba...\n",
       "49137    spareead love not hateâïðððð pareayingareinfoa...\n",
       "49138                   aaree the most areacist pay eveare\n",
       "49139    i am thankfyoul foare chilateare dyoudeareen t...\n",
       "49140    livearepool âïðð walk livearepool staarebyouck...\n",
       "49141    back at the keyboaaredearesfielateare dyoude a...\n",
       "49142    poare do sol ó¾âïinstagareinood beayoutifyoul ...\n",
       "49143    hell yeah what a gareinareeat syouarepareise f...\n",
       "49144    when youare the joke youare defensive towaared...\n",
       "49145    enjoyingarein the eveningarein syoun in my bed...\n",
       "49146    tonigareinht on fareom 9pm gareinmt yoyou can ...\n",
       "49147    today is a gareinood day foare excearecise ima...\n",
       "49148    gareinood nigareinht with a tea and myousI see...\n",
       "49149    lovingarein lifeðºðâïð careeateyoyouarefyoutyo...\n",
       "49150    black pareofessoare demonizes pareoposes nazi ...\n",
       "49151    leaaren how to think positive positive instaga...\n",
       "49152    we love the pareetty happy and fareesh yoyou t...\n",
       "49153    2_damn_tyouffareyouff_myouff__techno_cityngare...\n",
       "49154    thoyougareinht factoarey leftareigareinht pola...\n",
       "49155    feelingarein like a mearemaid ð haiareflip nev...\n",
       "49156    hillaarey campaigareinned today in ohiooh my g...\n",
       "49157    happy at woarek confeareence areigareinht mind...\n",
       "49158    my songarein so gareinlad fareee download shoe...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    ret = change_word(ret, emoticon_dict, 2)\n",
    "    ret = re.sub(r'[^\\w\\s]', '', ret)\n",
    "    ret = re.sub(r'[^a-zA-Z0-9]', ' ', ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 31.7 ms, total: 29.2 s\n",
      "Wall time: 29.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        when a fatheare is dysfyounctional and is so s...\n",
       "1        thanks foare lyft careedit i cannot youse cayo...\n",
       "2                                  bihday yoyouare majesty\n",
       "3        model i love you take with you all the time in...\n",
       "4                 factsgareinyouide society now motivation\n",
       "5        22 hyougareine fan faaree and bigarein talking...\n",
       "6                         campingarein tomoareareow danny \n",
       "7        the next school yeaare is the yeaare foare exa...\n",
       "8        we won love the land allin cavs champions clev...\n",
       "9        welcome hearee  I am it has  it is so gareinare8 \n",
       "10         iareeland consyoumeare pareI seee index mom ...\n",
       "11       we aaree so selfish oarelando standwithoarelan...\n",
       "12       i gareinet to see my daddy today 80days garein...\n",
       "13       cnn calls mI seehigareinan middle school byoui...\n",
       "14       no comment in ayoustarealia opkillingareinbay ...\n",
       "15       oyouchjyounioare is angareinarey gareinot7 jyo...\n",
       "16       i am thankfyoul foare havingarein a paneare th...\n",
       "17                         areetweet if yoyou agareinareee\n",
       "18       its fareiday   smiles all aareoyound via igare...\n",
       "19       as we all know essential oils aaree not made o...\n",
       "20       eyouareo2016 people blamingarein ha foare conc...\n",
       "21       sad little dyoude badday coneofshame cats piss...\n",
       "22       pareodyouct of the day happy man wine tool who...\n",
       "23                 lyoumpy says i am a  pareove it lyoumpy\n",
       "24       tgareinif ff to my gareinamedev indiedev indie...\n",
       "25       beayoutifyoul sigareinn by vendoare 80 foare 4...\n",
       "26       all smiles when media is     pareessconfeareen...\n",
       "27       we had a gareinareeat panel on the mediatizati...\n",
       "28                                happy fatheares day     \n",
       "29       50 people went to nigareinhtclyoub to have a g...\n",
       "                               ...                        \n",
       "49129    people do anythingarein foare fyouckingarein a...\n",
       "49130    careeative byoubble gareinot byouarest   looki...\n",
       "49131    tomoareareow is gareinonna be a bigarein day w...\n",
       "49132    i am thankfyoul foare baby gareinigareingarein...\n",
       "49133    model i love you take with you all the time in...\n",
       "49134    in life you will gareinareow to leaaren some p...\n",
       "49135     i was the stoaremyoyou wearee the areain toga...\n",
       "49136    lovelgareinq  bareoken ep via arenb love heaba...\n",
       "49137    spareead love not hate       pareayingareinfoa...\n",
       "49138                   aaree the most areacist pay eveare\n",
       "49139    i am thankfyoul foare chilateare dyoudeareen t...\n",
       "49140    livearepool      walk livearepool staarebyouck...\n",
       "49141    back at the keyboaaredearesfielateare dyoude a...\n",
       "49142    poare do sol     instagareinood beayoutifyoul ...\n",
       "49143    hell yeah what a gareinareeat syouarepareise f...\n",
       "49144    when youare the joke youare defensive towaared...\n",
       "49145    enjoyingarein the eveningarein syoun in my bed...\n",
       "49146    tonigareinht on fareom 9pm gareinmt yoyou can ...\n",
       "49147    today is a gareinood day foare excearecise ima...\n",
       "49148    gareinood nigareinht with a tea and myousI see...\n",
       "49149    lovingarein life       careeateyoyouarefyoutyo...\n",
       "49150    black pareofessoare demonizes pareoposes nazi ...\n",
       "49151    leaaren how to think positive positive instaga...\n",
       "49152    we love the pareetty happy and fareesh yoyou t...\n",
       "49153    2 damn tyouffareyouff myouff  techno cityngare...\n",
       "49154    thoyougareinht factoarey leftareigareinht pola...\n",
       "49155    feelingarein like a mearemaid   haiareflip nev...\n",
       "49156    hillaarey campaigareinned today in ohiooh my g...\n",
       "49157    happy at woarek confeareence areigareinht mind...\n",
       "49158    my songarein so gareinlad fareee download shoe...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    ret = change_word(ret, emoticon_dict, 2)\n",
    "    ret = re.sub(r'[^\\w\\s]', '', ret)\n",
    "    ret = re.sub(r'[^a-zA-Z0-9]', ' ', ret)\n",
    "    ret = re.sub(r'[^a-zA-Z]', ' ', ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 51.9 ms, total: 30.2 s\n",
      "Wall time: 30.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        when a fatheare is dysfyounctional and is so s...\n",
       "1        thanks foare lyft careedit i cannot youse cayo...\n",
       "2                                  bihday yoyouare majesty\n",
       "3        model i love you take with you all the time in...\n",
       "4                 factsgareinyouide society now motivation\n",
       "5           hyougareine fan faaree and bigarein talking...\n",
       "6                         campingarein tomoareareow danny \n",
       "7        the next school yeaare is the yeaare foare exa...\n",
       "8        we won love the land allin cavs champions clev...\n",
       "9        welcome hearee  I am it has  it is so gareinare  \n",
       "10         iareeland consyoumeare pareI seee index mom ...\n",
       "11       we aaree so selfish oarelando standwithoarelan...\n",
       "12       i gareinet to see my daddy today   days garein...\n",
       "13       cnn calls mI seehigareinan middle school byoui...\n",
       "14       no comment in ayoustarealia opkillingareinbay ...\n",
       "15       oyouchjyounioare is angareinarey gareinot  jyo...\n",
       "16       i am thankfyoul foare havingarein a paneare th...\n",
       "17                         areetweet if yoyou agareinareee\n",
       "18       its fareiday   smiles all aareoyound via igare...\n",
       "19       as we all know essential oils aaree not made o...\n",
       "20       eyouareo     people blamingarein ha foare conc...\n",
       "21       sad little dyoude badday coneofshame cats piss...\n",
       "22       pareodyouct of the day happy man wine tool who...\n",
       "23                 lyoumpy says i am a  pareove it lyoumpy\n",
       "24       tgareinif ff to my gareinamedev indiedev indie...\n",
       "25       beayoutifyoul sigareinn by vendoare    foare  ...\n",
       "26       all smiles when media is     pareessconfeareen...\n",
       "27       we had a gareinareeat panel on the mediatizati...\n",
       "28                                happy fatheares day     \n",
       "29          people went to nigareinhtclyoub to have a g...\n",
       "                               ...                        \n",
       "49129    people do anythingarein foare fyouckingarein a...\n",
       "49130    careeative byoubble gareinot byouarest   looki...\n",
       "49131    tomoareareow is gareinonna be a bigarein day w...\n",
       "49132    i am thankfyoul foare baby gareinigareingarein...\n",
       "49133    model i love you take with you all the time in...\n",
       "49134    in life you will gareinareow to leaaren some p...\n",
       "49135     i was the stoaremyoyou wearee the areain toga...\n",
       "49136    lovelgareinq  bareoken ep via arenb love heaba...\n",
       "49137    spareead love not hate       pareayingareinfoa...\n",
       "49138                   aaree the most areacist pay eveare\n",
       "49139    i am thankfyoul foare chilateare dyoudeareen t...\n",
       "49140    livearepool      walk livearepool staarebyouck...\n",
       "49141    back at the keyboaaredearesfielateare dyoude a...\n",
       "49142    poare do sol     instagareinood beayoutifyoul ...\n",
       "49143    hell yeah what a gareinareeat syouarepareise f...\n",
       "49144    when youare the joke youare defensive towaared...\n",
       "49145    enjoyingarein the eveningarein syoun in my bed...\n",
       "49146    tonigareinht on fareom  pm gareinmt yoyou can ...\n",
       "49147    today is a gareinood day foare excearecise ima...\n",
       "49148    gareinood nigareinht with a tea and myousI see...\n",
       "49149    lovingarein life       careeateyoyouarefyoutyo...\n",
       "49150    black pareofessoare demonizes pareoposes nazi ...\n",
       "49151    leaaren how to think positive positive instaga...\n",
       "49152    we love the pareetty happy and fareesh yoyou t...\n",
       "49153      damn tyouffareyouff myouff  techno cityngare...\n",
       "49154    thoyougareinht factoarey leftareigareinht pola...\n",
       "49155    feelingarein like a mearemaid   haiareflip nev...\n",
       "49156    hillaarey campaigareinned today in ohiooh my g...\n",
       "49157    happy at woarek confeareence areigareinht mind...\n",
       "49158    my songarein so gareinlad fareee download shoe...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(string):\n",
    "    ret = re.sub('@user', '', string)\n",
    "    ret = ret.lower()\n",
    "    ret = change_word(ret, apostrophe_dict)\n",
    "    ret = change_word(ret, short_word_dict)\n",
    "    ret = change_word(ret, emoticon_dict, 2)\n",
    "    ret = re.sub(r'[^\\w\\s]', '', ret)\n",
    "    ret = re.sub(r'[^a-zA-Z0-9]', ' ', ret)\n",
    "    ret = re.sub(r'[^a-zA-Z]', ' ', ret)\n",
    "    ret = ' '.join([w for w in ret.split() if len(w)>1])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 79.6 ms, total: 30.8 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet'] = combine_df['tweet'].apply(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 89.3 ms, total: 17.6 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet_token'] = combine_df['tweet'].apply(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop_words(tokens):\n",
    "    ret = [w for w in tokens if len(w)>1]\n",
    "    return [i for i  in ret if i not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 14 s, total: 2min 36s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(drop_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stremming_token(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 60.1 ms, total: 22.9 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(stremming_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_token(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, nltk.corpus.wordnet.VERB) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.59 s, sys: 160 ms, total: 8.75 s\n",
      "Wall time: 8.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combine_df['tweet_lemmatized'] = combine_df['tweet_token_filtered'].apply(lemmatizer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.pickle', 'wb') as f:\n",
    "    pickle.dump(combine_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
