{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when fatheare is dysfyounctional and is so sel...</td>\n",
       "      <td>[when, fatheare, is, dysfyounctional, and, is,...</td>\n",
       "      <td>[fatheare, dysfyounctional, selfish, dareagare...</td>\n",
       "      <td>[fathear, dysfyounct, selfish, dareagarein, ki...</td>\n",
       "      <td>[fatheare, dysfyounctional, selfish, dareagare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks foare lyft careedit cannot youse cayous...</td>\n",
       "      <td>[thanks, foare, lyft, careedit, can, not, yous...</td>\n",
       "      <td>[thanks, foare, lyft, careedit, youse, cayouse...</td>\n",
       "      <td>[thank, foar, lyft, careedit, yous, cayous, of...</td>\n",
       "      <td>[thank, foare, lyft, careedit, youse, cayouse,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday yoyouare majesty</td>\n",
       "      <td>[bihday, yoyouare, majesty]</td>\n",
       "      <td>[bihday, yoyouare, majesty]</td>\n",
       "      <td>[bihday, yoyouar, majesti]</td>\n",
       "      <td>[bihday, yoyouare, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>model love you take with you all the time in y...</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, youare]</td>\n",
       "      <td>[model, love, take, time, youar]</td>\n",
       "      <td>[model, love, take, time, youare]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsgareinyouide society now motivation</td>\n",
       "      <td>[factsgareinyouide, society, now, motivation]</td>\n",
       "      <td>[factsgareinyouide, society, motivation]</td>\n",
       "      <td>[factsgareinyouid, societi, motiv]</td>\n",
       "      <td>[factsgareinyouide, society, motivation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thoyougareinht factoarey leftareigareinht pola...</td>\n",
       "      <td>[thoyougareinht, factoarey, leftareigareinht, ...</td>\n",
       "      <td>[thoyougareinht, factoarey, leftareigareinht, ...</td>\n",
       "      <td>[thoyougareinht, factoarey, leftareigareinht, ...</td>\n",
       "      <td>[thoyougareinht, factoarey, leftareigareinht, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feelingarein like mearemaid haiareflip neveare...</td>\n",
       "      <td>[feelingarein, like, mearemaid, haiareflip, ne...</td>\n",
       "      <td>[feelingarein, like, mearemaid, haiareflip, ne...</td>\n",
       "      <td>[feelingarein, like, mearemaid, haiareflip, ne...</td>\n",
       "      <td>[feelingarein, like, mearemaid, haiareflip, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hillaarey campaigareinned today in ohiooh my g...</td>\n",
       "      <td>[hillaarey, campaigareinned, today, in, ohiooh...</td>\n",
       "      <td>[hillaarey, campaigareinned, today, ohiooh, go...</td>\n",
       "      <td>[hillaarey, campaigarein, today, ohiooh, godar...</td>\n",
       "      <td>[hillaarey, campaigareinned, today, ohiooh, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy at woarek confeareence areigareinht mind...</td>\n",
       "      <td>[happy, at, woarek, confeareence, areigareinht...</td>\n",
       "      <td>[happy, woarek, confeareence, areigareinht, mi...</td>\n",
       "      <td>[happi, woarek, confear, areigareinht, mindset...</td>\n",
       "      <td>[happy, woarek, confeareence, areigareinht, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my songarein so gareinlad fareee download shoe...</td>\n",
       "      <td>[my, songarein, so, gareinlad, fareee, downloa...</td>\n",
       "      <td>[songarein, gareinlad, fareee, download, shoeg...</td>\n",
       "      <td>[songarein, gareinlad, faree, download, shoega...</td>\n",
       "      <td>[songarein, gareinlad, fareee, download, shoeg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1    0.0  when fatheare is dysfyounctional and is so sel...   \n",
       "1          2    0.0  thanks foare lyft careedit cannot youse cayous...   \n",
       "2          3    0.0                            bihday yoyouare majesty   \n",
       "3          4    0.0  model love you take with you all the time in y...   \n",
       "4          5    0.0           factsgareinyouide society now motivation   \n",
       "...      ...    ...                                                ...   \n",
       "49154  49155    NaN  thoyougareinht factoarey leftareigareinht pola...   \n",
       "49155  49156    NaN  feelingarein like mearemaid haiareflip neveare...   \n",
       "49156  49157    NaN  hillaarey campaigareinned today in ohiooh my g...   \n",
       "49157  49158    NaN  happy at woarek confeareence areigareinht mind...   \n",
       "49158  49159    NaN  my songarein so gareinlad fareee download shoe...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "0      [when, fatheare, is, dysfyounctional, and, is,...   \n",
       "1      [thanks, foare, lyft, careedit, can, not, yous...   \n",
       "2                            [bihday, yoyouare, majesty]   \n",
       "3      [model, love, you, take, with, you, all, the, ...   \n",
       "4          [factsgareinyouide, society, now, motivation]   \n",
       "...                                                  ...   \n",
       "49154  [thoyougareinht, factoarey, leftareigareinht, ...   \n",
       "49155  [feelingarein, like, mearemaid, haiareflip, ne...   \n",
       "49156  [hillaarey, campaigareinned, today, in, ohiooh...   \n",
       "49157  [happy, at, woarek, confeareence, areigareinht...   \n",
       "49158  [my, songarein, so, gareinlad, fareee, downloa...   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "0      [fatheare, dysfyounctional, selfish, dareagare...   \n",
       "1      [thanks, foare, lyft, careedit, youse, cayouse...   \n",
       "2                            [bihday, yoyouare, majesty]   \n",
       "3                      [model, love, take, time, youare]   \n",
       "4               [factsgareinyouide, society, motivation]   \n",
       "...                                                  ...   \n",
       "49154  [thoyougareinht, factoarey, leftareigareinht, ...   \n",
       "49155  [feelingarein, like, mearemaid, haiareflip, ne...   \n",
       "49156  [hillaarey, campaigareinned, today, ohiooh, go...   \n",
       "49157  [happy, woarek, confeareence, areigareinht, mi...   \n",
       "49158  [songarein, gareinlad, fareee, download, shoeg...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "0      [fathear, dysfyounct, selfish, dareagarein, ki...   \n",
       "1      [thank, foar, lyft, careedit, yous, cayous, of...   \n",
       "2                             [bihday, yoyouar, majesti]   \n",
       "3                       [model, love, take, time, youar]   \n",
       "4                     [factsgareinyouid, societi, motiv]   \n",
       "...                                                  ...   \n",
       "49154  [thoyougareinht, factoarey, leftareigareinht, ...   \n",
       "49155  [feelingarein, like, mearemaid, haiareflip, ne...   \n",
       "49156  [hillaarey, campaigarein, today, ohiooh, godar...   \n",
       "49157  [happi, woarek, confear, areigareinht, mindset...   \n",
       "49158  [songarein, gareinlad, faree, download, shoega...   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "0      [fatheare, dysfyounctional, selfish, dareagare...  \n",
       "1      [thank, foare, lyft, careedit, youse, cayouse,...  \n",
       "2                            [bihday, yoyouare, majesty]  \n",
       "3                      [model, love, take, time, youare]  \n",
       "4               [factsgareinyouide, society, motivation]  \n",
       "...                                                  ...  \n",
       "49154  [thoyougareinht, factoarey, leftareigareinht, ...  \n",
       "49155  [feelingarein, like, mearemaid, haiareflip, ne...  \n",
       "49156  [hillaarey, campaigareinned, today, ohiooh, go...  \n",
       "49157  [happy, woarek, confeareence, areigareinht, mi...  \n",
       "49158  [songarein, gareinlad, fareee, download, shoeg...  \n",
       "\n",
       "[49159 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../lesson_1/result.pickle', 'rb') as f:\n",
    "    combine_df = pickle.load(f)\n",
    "combine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1.\n",
    "Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000)\n",
    "С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?\n",
    "Повторим шаги из заданий 1 и 2, используя библиотеку nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_dict = {}\n",
    "for tweet in combine_df['tweet'].values:\n",
    "    for entity in nlp(tweet).ents:\n",
    "        if entity.label_ in ents_dict.keys():\n",
    "            ents_dict[entity.label_] += 1\n",
    "        else:\n",
    "            ents_dict[entity.label_] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PERSON', 23831),\n",
       " ('ORG', 8761),\n",
       " ('DATE', 7305),\n",
       " ('GPE', 3972),\n",
       " ('CARDINAL', 1756),\n",
       " ('NORP', 1235),\n",
       " ('FAC', 1116),\n",
       " ('PRODUCT', 1018),\n",
       " ('LOC', 174),\n",
       " ('TIME', 91),\n",
       " ('ORDINAL', 81),\n",
       " ('EVENT', 77),\n",
       " ('QUANTITY', 68),\n",
       " ('WORK_OF_ART', 48),\n",
       " ('LAW', 16),\n",
       " ('MONEY', 13),\n",
       " ('LANGUAGE', 7),\n",
       " ('PERCENT', 4)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ents_dict.items(), key=lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksandr/anaconda3/lib/python3.7/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    }
   ],
   "source": [
    "# Реализовал немного по другому, но можно было и в первом варианте считать\n",
    "ents_dict = {}\n",
    "for tweet in combine_df['tweet'].values:\n",
    "    article_disp = displacy.parse_ents(nlp(tweet))\n",
    "    ents = article_disp['ents']\n",
    "    text = article_disp['text']\n",
    "    if len(ents) > 0:\n",
    "        for ent in ents:\n",
    "            label = ent['label']\n",
    "            word = text[ent['start']:ent['end']]\n",
    "            if label in ents_dict.keys(): \n",
    "                ents_dict[label]['count'] += 1\n",
    "                if word in ents_dict[label]['words'].keys():\n",
    "                    ents_dict[label]['words'][word] += 1\n",
    "                else:\n",
    "                    ents_dict[label]['words'][word] = 1\n",
    "            else:\n",
    "                ents_dict[label] = {'count': 1, 'words': {word: 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('woarelateare', 294)\n",
      "('sjw', 100)\n",
      "('lookingarein foarewaared', 74)\n",
      "('nigareinht', 73)\n",
      "('app', 65)\n",
      "('gareinone', 60)\n",
      "('islam', 49)\n",
      "('beingarein', 44)\n",
      "('byouy thingareins', 43)\n",
      "('youpdate social analytI', 40)\n",
      "('feminismiscanceare', 40)\n",
      "('eveare', 37)\n",
      "('nba', 34)\n",
      "('gareinift', 32)\n",
      "('aaree yoyou black amp', 30)\n",
      "('myousI', 29)\n",
      "('cnn', 27)\n",
      "('wishingarein', 27)\n",
      "('whearee', 27)\n",
      "('hearee', 27)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for org in sorted(ents_dict['ORG']['words'].items(), key=lambda item: item[1], reverse = True):\n",
    "    i += 1\n",
    "    print(org)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('youp yoyou', 275)\n",
      "('gareinoingarein', 200)\n",
      "('youp', 152)\n",
      "('gareinood moareningarein', 140)\n",
      "('syoummeare', 114)\n",
      "('areacingarein angareinarey polaare beaare', 112)\n",
      "('yoyou migareinht', 107)\n",
      "('bongarein bingarein', 107)\n",
      "('hea', 106)\n",
      "('tarey', 95)\n",
      "('moareningarein', 94)\n",
      "('blogarein silveare gareinolateare', 90)\n",
      "('byouffalo foare', 75)\n",
      "('gareinettingarein', 64)\n",
      "('lookingarein', 63)\n",
      "('heare', 61)\n",
      "('stoarey', 58)\n",
      "('missingarein', 55)\n",
      "('syoummeare blyouare', 52)\n",
      "('foare beingarein', 51)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for person in sorted(ents_dict['PERSON']['words'].items(), key=lambda item: item[1], reverse = True):\n",
    "    i += 1\n",
    "    print(person)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "Используя библиотеку nltk, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? Для данного задания используем ограничение на количество символов во входном датасете (max_word_limit_spacy = 1000000), чтобы иметь возможность сравнить результаты работы Spacy и nltk. Обратите внимание, что nltk чувствителен к регистру.\n",
    "С помощью nltk выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we won love the land allin cavs champions cleveland clevelandcavalieares\n",
      "WE WON LOVE THE LAND ALLIN CAVS CHAMPIONS CLEVELAND CLEVELANDCAVALIEARES\n",
      "('LAND', 'ORGANIZATION')\n"
     ]
    }
   ],
   "source": [
    "document = combine_df['tweet'].values[8]\n",
    "print(document)\n",
    "document = str(document).upper()\n",
    "print(document)\n",
    "a = [(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') ]\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_dict_nltk = {}\n",
    "ents_dict_nltk_perc = {}\n",
    "for document in combine_df['tweet'].values:\n",
    "    entitys = [(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') ]\n",
    "    if len(entitys) > 0:\n",
    "        for entity in entitys:\n",
    "            if entity[1] in ents_dict_nltk_perc.keys():\n",
    "                ents_dict_nltk[entity[1]] += 1\n",
    "                if entity[0] in ents_dict_nltk_perc[entity[1]].keys():\n",
    "                    ents_dict_nltk_perc[entity[1]][entity[0]] += 1\n",
    "                else:\n",
    "                    ents_dict_nltk_perc[entity[1]][entity[0]] = 1\n",
    "            else:\n",
    "                ents_dict_nltk[entity[1]] = 1 \n",
    "                ents_dict_nltk_perc = {entity[1]: {entity[0]: 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ORGANIZATION', 31), ('GPE', 1), ('GSP', 1)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ents_dict_nltk.items(), key=lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('myousI', 5)\n",
      "('careitI', 2)\n",
      "('vI', 1)\n",
      "('typI', 1)\n",
      "('twI', 1)\n",
      "('syouI', 1)\n",
      "('vlI', 1)\n",
      "('naarecissistI', 1)\n",
      "('chI', 1)\n",
      "('nohameareI', 1)\n",
      "('impathetI', 1)\n",
      "('tlpetsearevI', 1)\n",
      "('misogareinynistI', 1)\n",
      "('ceifI', 1)\n",
      "('cychotI', 1)\n",
      "('bandI', 1)\n",
      "('offI', 1)\n",
      "('careazygareinoodvoI', 1)\n",
      "('eareI', 1)\n",
      "('whI', 1)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for person in sorted(ents_dict_nltk_perc['ORGANIZATION'].items(), key=lambda item: item[1], reverse = True):\n",
    "    i += 1\n",
    "    print(person)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 3.\n",
    "Какая из библиотек по вашему лучше отработала? Сравните качество полученных most_common NER и количество распознаных NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy на мой взгляд отработала лучше, больше распознаных NER и их качество лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
